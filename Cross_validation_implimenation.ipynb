{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cross validation implimenation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1lXdWMrwfTfkkDRCXHPI2tkzAfQoJt10v",
      "authorship_tag": "ABX9TyPK4wYNm/Z1nIZYF9Zi0+Br",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tohsin/Deeplearning/blob/master/Cross_validation_implimenation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYyr6Rg9h-Kf"
      },
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPo7t6knbw4e"
      },
      "source": [
        "import numpy as np\n",
        "from numpy import genfromtxt\n",
        "\n",
        "my_data=genfromtxt('/content/drive/My Drive/Colab Notebooks/data_file.csv',delimiter=',')\n",
        "\n",
        "new_array=np.delete(my_data ,0,0 )\n",
        "\n",
        "#a place to store the label or target value\n",
        "label=[]\n",
        "\n",
        "\n",
        "for element in new_array:\n",
        "    label.append(element[-1])\n",
        "\n",
        "data=np.delete(new_array,8,axis=1)\n",
        "tensor_label=np.asarray(label)\n",
        "\n",
        "#sharing the data 90 to 10 percent \n",
        "train_data=data[:615]\n",
        "train_targets=tensor_label[:615]\n",
        "\n",
        "test_data=data[615:]\n",
        "test_targets=tensor_label[:615]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQoDA0T7b-xH"
      },
      "source": [
        "#using manual method to try to normalise the data and put the values between 0 and 1\n",
        "mean=train_data.mean(axis=0)\n",
        "std = train_data.std(axis=0)\n",
        "\n",
        "train_data -= mean\n",
        "train_data /=std\n",
        "\n",
        "test_data -=mean\n",
        "test_data/= std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgBq9XY-cBf_"
      },
      "source": [
        "from tensorflow.keras import models, layers\n",
        "\n",
        "#funtion to build model\n",
        "def build_model():\n",
        "    model=models.Sequential()\n",
        "    model.add(layers.Dense(216, activation='relu', input_shape=(train_data.shape[1],)))\n",
        "    #adding noise with not a lot of dropout to reduce overfitting\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    model.add(layers.Dense(512 ,activation='relu'))\n",
        "    model.add(layers.Dense(1,activation='sigmoid'))\n",
        "    model.compile(\n",
        "        optimizer='rmsprop',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['acc'])\n",
        "    #used optimizer  funtion of rmsprop ,loss of binary cross entropy\n",
        "    #since to just compare two values a or b and metric of accuracy as means of measurement\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8SKsixpcHim",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "83f91526-97ee-4783-def6-dbaa265e2b79"
      },
      "source": [
        "#decided to use k fold validation as the data isnt large enough for regular \n",
        "#data classification\n",
        "k=4\n",
        "num_val_samples=len(train_data) // k\n",
        "num_of_epochs=100\n",
        "accuracy_val=[]\n",
        "for i in range (k):\n",
        "  word=\"running process {}/{} :\"\n",
        "  print(word.format((i+1),k))\n",
        "\n",
        "  val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples] \n",
        "  val_target=train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "\n",
        "  #remianing data\n",
        "  partial_train_data=np.concatenate(\n",
        "      [train_data[:i*num_val_samples],\n",
        "      train_data[(i+1) *num_val_samples:]],\n",
        "      axis=0)\n",
        " \n",
        "  partial_train_targets=np.concatenate(\n",
        "      [train_targets[:i*num_val_samples],\n",
        "      train_targets[(i+1) *num_val_samples:]],\n",
        "      axis=0)\n",
        "  model=build_model()\n",
        "  history=model.fit(partial_train_data, partial_train_targets, \n",
        "                      validation_data=(val_data,val_target),   \n",
        "              epochs=num_of_epochs, batch_size=1)\n",
        "  accuracy=history.history['acc']\n",
        "  accuracy_val.append(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running process 1/4 :\n",
            "Epoch 1/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 4.6883 - acc: 0.5844 - val_loss: 2.9197 - val_acc: 0.6471\n",
            "Epoch 2/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 1.8631 - acc: 0.6190 - val_loss: 1.5402 - val_acc: 0.5098\n",
            "Epoch 3/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 1.2574 - acc: 0.6082 - val_loss: 0.8839 - val_acc: 0.4837\n",
            "Epoch 4/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.9559 - acc: 0.6364 - val_loss: 1.0246 - val_acc: 0.6667\n",
            "Epoch 5/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.9210 - acc: 0.6558 - val_loss: 0.7432 - val_acc: 0.6471\n",
            "Epoch 6/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.9236 - acc: 0.6472 - val_loss: 0.6930 - val_acc: 0.5752\n",
            "Epoch 7/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.8152 - acc: 0.6775 - val_loss: 0.7337 - val_acc: 0.6144\n",
            "Epoch 8/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.8283 - acc: 0.6602 - val_loss: 0.8837 - val_acc: 0.6863\n",
            "Epoch 9/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.8248 - acc: 0.6905 - val_loss: 0.8115 - val_acc: 0.5882\n",
            "Epoch 10/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8330 - acc: 0.6342 - val_loss: 0.6842 - val_acc: 0.6667\n",
            "Epoch 11/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.7841 - acc: 0.6732 - val_loss: 1.0037 - val_acc: 0.6732\n",
            "Epoch 12/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7520 - acc: 0.6905 - val_loss: 0.7115 - val_acc: 0.6340\n",
            "Epoch 13/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7778 - acc: 0.6667 - val_loss: 0.7480 - val_acc: 0.6667\n",
            "Epoch 14/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7031 - acc: 0.6710 - val_loss: 0.8226 - val_acc: 0.6667\n",
            "Epoch 15/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8017 - acc: 0.6797 - val_loss: 0.7422 - val_acc: 0.5294\n",
            "Epoch 16/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7121 - acc: 0.6688 - val_loss: 0.7173 - val_acc: 0.6863\n",
            "Epoch 17/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7261 - acc: 0.6883 - val_loss: 0.6634 - val_acc: 0.6797\n",
            "Epoch 18/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7703 - acc: 0.6818 - val_loss: 0.7209 - val_acc: 0.6667\n",
            "Epoch 19/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7764 - acc: 0.6623 - val_loss: 0.6628 - val_acc: 0.6667\n",
            "Epoch 20/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7434 - acc: 0.6753 - val_loss: 0.7250 - val_acc: 0.6601\n",
            "Epoch 21/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.7603 - acc: 0.6818 - val_loss: 0.6795 - val_acc: 0.6667\n",
            "Epoch 22/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.7499 - acc: 0.6861 - val_loss: 0.6324 - val_acc: 0.6601\n",
            "Epoch 23/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.7489 - acc: 0.6710 - val_loss: 0.8426 - val_acc: 0.7124\n",
            "Epoch 24/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.7452 - acc: 0.6797 - val_loss: 0.6984 - val_acc: 0.6797\n",
            "Epoch 25/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.7767 - acc: 0.6948 - val_loss: 0.8005 - val_acc: 0.6601\n",
            "Epoch 26/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.7218 - acc: 0.6840 - val_loss: 0.7163 - val_acc: 0.6471\n",
            "Epoch 27/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7548 - acc: 0.6905 - val_loss: 0.6795 - val_acc: 0.6993\n",
            "Epoch 28/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7396 - acc: 0.6602 - val_loss: 0.7359 - val_acc: 0.6863\n",
            "Epoch 29/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7434 - acc: 0.6840 - val_loss: 0.7200 - val_acc: 0.6797\n",
            "Epoch 30/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.7088 - acc: 0.7056 - val_loss: 0.7589 - val_acc: 0.7190\n",
            "Epoch 31/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.8314 - acc: 0.6948 - val_loss: 0.7653 - val_acc: 0.6928\n",
            "Epoch 32/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7297 - acc: 0.6775 - val_loss: 0.7552 - val_acc: 0.6471\n",
            "Epoch 33/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.7763 - acc: 0.7013 - val_loss: 0.7919 - val_acc: 0.6536\n",
            "Epoch 34/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7381 - acc: 0.7186 - val_loss: 0.7904 - val_acc: 0.6667\n",
            "Epoch 35/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7698 - acc: 0.6905 - val_loss: 0.6700 - val_acc: 0.6732\n",
            "Epoch 36/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7599 - acc: 0.6775 - val_loss: 0.8112 - val_acc: 0.6667\n",
            "Epoch 37/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.8038 - acc: 0.6948 - val_loss: 0.7527 - val_acc: 0.6667\n",
            "Epoch 38/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7773 - acc: 0.7165 - val_loss: 0.9456 - val_acc: 0.6667\n",
            "Epoch 39/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7973 - acc: 0.6537 - val_loss: 1.0327 - val_acc: 0.6471\n",
            "Epoch 40/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.7487 - acc: 0.6970 - val_loss: 0.8432 - val_acc: 0.6340\n",
            "Epoch 41/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.7826 - acc: 0.6926 - val_loss: 0.7048 - val_acc: 0.6536\n",
            "Epoch 42/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8371 - acc: 0.6818 - val_loss: 0.6796 - val_acc: 0.6667\n",
            "Epoch 43/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8279 - acc: 0.6948 - val_loss: 0.8788 - val_acc: 0.6732\n",
            "Epoch 44/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.7722 - acc: 0.6840 - val_loss: 0.8752 - val_acc: 0.6667\n",
            "Epoch 45/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.7447 - acc: 0.6775 - val_loss: 0.7730 - val_acc: 0.6536\n",
            "Epoch 46/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7516 - acc: 0.6970 - val_loss: 0.7841 - val_acc: 0.7059\n",
            "Epoch 47/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.8196 - acc: 0.6840 - val_loss: 0.7769 - val_acc: 0.5686\n",
            "Epoch 48/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8395 - acc: 0.6580 - val_loss: 0.6707 - val_acc: 0.6732\n",
            "Epoch 49/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7648 - acc: 0.6926 - val_loss: 0.7672 - val_acc: 0.6601\n",
            "Epoch 50/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7776 - acc: 0.6991 - val_loss: 0.7054 - val_acc: 0.6863\n",
            "Epoch 51/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.7704 - acc: 0.6970 - val_loss: 1.0851 - val_acc: 0.6732\n",
            "Epoch 52/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8027 - acc: 0.6926 - val_loss: 1.0010 - val_acc: 0.6536\n",
            "Epoch 53/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8458 - acc: 0.6970 - val_loss: 0.8317 - val_acc: 0.6863\n",
            "Epoch 54/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.7764 - acc: 0.6710 - val_loss: 0.9585 - val_acc: 0.7059\n",
            "Epoch 55/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7806 - acc: 0.6818 - val_loss: 0.8247 - val_acc: 0.6667\n",
            "Epoch 56/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.8100 - acc: 0.7056 - val_loss: 0.7815 - val_acc: 0.6667\n",
            "Epoch 57/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.7970 - acc: 0.6948 - val_loss: 0.9135 - val_acc: 0.6797\n",
            "Epoch 58/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.8209 - acc: 0.6753 - val_loss: 0.8971 - val_acc: 0.6928\n",
            "Epoch 59/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.8388 - acc: 0.6840 - val_loss: 0.7423 - val_acc: 0.6340\n",
            "Epoch 60/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.8028 - acc: 0.6818 - val_loss: 1.0249 - val_acc: 0.6732\n",
            "Epoch 61/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.8603 - acc: 0.6753 - val_loss: 0.7993 - val_acc: 0.5229\n",
            "Epoch 62/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.7785 - acc: 0.6385 - val_loss: 0.7894 - val_acc: 0.6797\n",
            "Epoch 63/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.8406 - acc: 0.6710 - val_loss: 0.7263 - val_acc: 0.6667\n",
            "Epoch 64/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.7840 - acc: 0.6861 - val_loss: 0.9063 - val_acc: 0.6601\n",
            "Epoch 65/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7921 - acc: 0.6688 - val_loss: 0.9332 - val_acc: 0.6863\n",
            "Epoch 66/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8287 - acc: 0.7013 - val_loss: 1.2394 - val_acc: 0.6797\n",
            "Epoch 67/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.8232 - acc: 0.7229 - val_loss: 0.8648 - val_acc: 0.6471\n",
            "Epoch 68/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.7920 - acc: 0.6883 - val_loss: 1.2247 - val_acc: 0.6863\n",
            "Epoch 69/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.9229 - acc: 0.6558 - val_loss: 0.9316 - val_acc: 0.6667\n",
            "Epoch 70/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8074 - acc: 0.6883 - val_loss: 0.7969 - val_acc: 0.6275\n",
            "Epoch 71/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8572 - acc: 0.6775 - val_loss: 0.9228 - val_acc: 0.6667\n",
            "Epoch 72/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8383 - acc: 0.6710 - val_loss: 0.8360 - val_acc: 0.6993\n",
            "Epoch 73/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.8353 - acc: 0.6450 - val_loss: 0.8981 - val_acc: 0.4967\n",
            "Epoch 74/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8258 - acc: 0.6558 - val_loss: 0.8851 - val_acc: 0.6013\n",
            "Epoch 75/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8039 - acc: 0.6688 - val_loss: 1.7658 - val_acc: 0.6732\n",
            "Epoch 76/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8780 - acc: 0.6732 - val_loss: 0.8057 - val_acc: 0.6797\n",
            "Epoch 77/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8536 - acc: 0.6732 - val_loss: 0.9647 - val_acc: 0.6667\n",
            "Epoch 78/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7982 - acc: 0.6688 - val_loss: 0.7484 - val_acc: 0.6667\n",
            "Epoch 79/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7847 - acc: 0.7013 - val_loss: 0.8159 - val_acc: 0.6405\n",
            "Epoch 80/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8064 - acc: 0.6775 - val_loss: 0.8512 - val_acc: 0.6928\n",
            "Epoch 81/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7792 - acc: 0.7013 - val_loss: 0.9129 - val_acc: 0.5490\n",
            "Epoch 82/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7481 - acc: 0.6926 - val_loss: 1.3254 - val_acc: 0.7059\n",
            "Epoch 83/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8190 - acc: 0.6818 - val_loss: 0.8424 - val_acc: 0.6601\n",
            "Epoch 84/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 1.0063 - acc: 0.6688 - val_loss: 0.7402 - val_acc: 0.6536\n",
            "Epoch 85/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8373 - acc: 0.7013 - val_loss: 0.8730 - val_acc: 0.6863\n",
            "Epoch 86/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8286 - acc: 0.7100 - val_loss: 1.0434 - val_acc: 0.6667\n",
            "Epoch 87/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8037 - acc: 0.7013 - val_loss: 0.9146 - val_acc: 0.6601\n",
            "Epoch 88/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.9104 - acc: 0.6818 - val_loss: 0.9007 - val_acc: 0.6732\n",
            "Epoch 89/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8327 - acc: 0.6753 - val_loss: 0.9132 - val_acc: 0.6863\n",
            "Epoch 90/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8610 - acc: 0.7143 - val_loss: 0.8649 - val_acc: 0.6732\n",
            "Epoch 91/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7667 - acc: 0.7186 - val_loss: 1.0869 - val_acc: 0.6863\n",
            "Epoch 92/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.8761 - acc: 0.6970 - val_loss: 0.9022 - val_acc: 0.6667\n",
            "Epoch 93/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.9076 - acc: 0.6991 - val_loss: 1.1581 - val_acc: 0.6797\n",
            "Epoch 94/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8699 - acc: 0.6883 - val_loss: 0.9718 - val_acc: 0.6797\n",
            "Epoch 95/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8697 - acc: 0.6753 - val_loss: 0.8613 - val_acc: 0.6732\n",
            "Epoch 96/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8611 - acc: 0.6753 - val_loss: 0.9388 - val_acc: 0.7190\n",
            "Epoch 97/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.7516 - acc: 0.6818 - val_loss: 1.0974 - val_acc: 0.6797\n",
            "Epoch 98/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8672 - acc: 0.6515 - val_loss: 0.8788 - val_acc: 0.6601\n",
            "Epoch 99/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8764 - acc: 0.6840 - val_loss: 0.9772 - val_acc: 0.6601\n",
            "Epoch 100/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7926 - acc: 0.6970 - val_loss: 0.9874 - val_acc: 0.7320\n",
            "running process 2/4 :\n",
            "Epoch 1/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 3.9961 - acc: 0.5931 - val_loss: 1.9877 - val_acc: 0.6536\n",
            "Epoch 2/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 1.6374 - acc: 0.6364 - val_loss: 1.0605 - val_acc: 0.5817\n",
            "Epoch 3/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 1.3125 - acc: 0.6364 - val_loss: 1.5872 - val_acc: 0.5294\n",
            "Epoch 4/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.9585 - acc: 0.6753 - val_loss: 1.0331 - val_acc: 0.6144\n",
            "Epoch 5/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.9669 - acc: 0.6602 - val_loss: 1.4181 - val_acc: 0.4706\n",
            "Epoch 6/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8327 - acc: 0.6342 - val_loss: 0.9296 - val_acc: 0.6209\n",
            "Epoch 7/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7687 - acc: 0.6558 - val_loss: 1.2529 - val_acc: 0.5752\n",
            "Epoch 8/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7839 - acc: 0.6710 - val_loss: 1.2053 - val_acc: 0.6471\n",
            "Epoch 9/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8121 - acc: 0.6580 - val_loss: 0.7703 - val_acc: 0.6471\n",
            "Epoch 10/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7943 - acc: 0.6450 - val_loss: 0.8757 - val_acc: 0.5882\n",
            "Epoch 11/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7514 - acc: 0.6580 - val_loss: 0.8489 - val_acc: 0.6078\n",
            "Epoch 12/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7278 - acc: 0.6948 - val_loss: 1.0476 - val_acc: 0.6340\n",
            "Epoch 13/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7706 - acc: 0.6450 - val_loss: 0.8258 - val_acc: 0.6601\n",
            "Epoch 14/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7394 - acc: 0.6732 - val_loss: 0.8208 - val_acc: 0.6340\n",
            "Epoch 15/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7229 - acc: 0.6991 - val_loss: 0.7921 - val_acc: 0.6601\n",
            "Epoch 16/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7727 - acc: 0.6667 - val_loss: 0.6623 - val_acc: 0.6536\n",
            "Epoch 17/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7649 - acc: 0.7078 - val_loss: 0.8501 - val_acc: 0.6667\n",
            "Epoch 18/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7412 - acc: 0.6775 - val_loss: 0.8398 - val_acc: 0.6405\n",
            "Epoch 19/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7541 - acc: 0.6775 - val_loss: 0.9309 - val_acc: 0.6405\n",
            "Epoch 20/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7626 - acc: 0.6602 - val_loss: 1.0198 - val_acc: 0.6863\n",
            "Epoch 21/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7170 - acc: 0.6970 - val_loss: 1.2589 - val_acc: 0.6144\n",
            "Epoch 22/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.7676 - acc: 0.6883 - val_loss: 1.0473 - val_acc: 0.6732\n",
            "Epoch 23/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7085 - acc: 0.6926 - val_loss: 0.7793 - val_acc: 0.6536\n",
            "Epoch 24/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7061 - acc: 0.6926 - val_loss: 1.0134 - val_acc: 0.6667\n",
            "Epoch 25/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.7452 - acc: 0.6970 - val_loss: 0.9121 - val_acc: 0.6275\n",
            "Epoch 26/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7803 - acc: 0.6926 - val_loss: 1.0090 - val_acc: 0.6209\n",
            "Epoch 27/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7811 - acc: 0.6775 - val_loss: 0.7766 - val_acc: 0.6275\n",
            "Epoch 28/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.7952 - acc: 0.7035 - val_loss: 0.6801 - val_acc: 0.5817\n",
            "Epoch 29/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8028 - acc: 0.6732 - val_loss: 0.9426 - val_acc: 0.5817\n",
            "Epoch 30/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7670 - acc: 0.6818 - val_loss: 1.1164 - val_acc: 0.6601\n",
            "Epoch 31/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7386 - acc: 0.6710 - val_loss: 1.0095 - val_acc: 0.6863\n",
            "Epoch 32/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7874 - acc: 0.6861 - val_loss: 0.8522 - val_acc: 0.6013\n",
            "Epoch 33/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8069 - acc: 0.6710 - val_loss: 0.7725 - val_acc: 0.6601\n",
            "Epoch 34/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7032 - acc: 0.6775 - val_loss: 0.9852 - val_acc: 0.6863\n",
            "Epoch 35/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7550 - acc: 0.6558 - val_loss: 0.9338 - val_acc: 0.6013\n",
            "Epoch 36/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7598 - acc: 0.6797 - val_loss: 1.1333 - val_acc: 0.6144\n",
            "Epoch 37/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7549 - acc: 0.7100 - val_loss: 0.8461 - val_acc: 0.5359\n",
            "Epoch 38/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7831 - acc: 0.6710 - val_loss: 0.9274 - val_acc: 0.6144\n",
            "Epoch 39/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7663 - acc: 0.6732 - val_loss: 0.9232 - val_acc: 0.6275\n",
            "Epoch 40/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7944 - acc: 0.6667 - val_loss: 0.7294 - val_acc: 0.6601\n",
            "Epoch 41/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7361 - acc: 0.6861 - val_loss: 0.8993 - val_acc: 0.6209\n",
            "Epoch 42/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8019 - acc: 0.6861 - val_loss: 0.8656 - val_acc: 0.6144\n",
            "Epoch 43/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7592 - acc: 0.7208 - val_loss: 0.9995 - val_acc: 0.6405\n",
            "Epoch 44/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7730 - acc: 0.7013 - val_loss: 1.1542 - val_acc: 0.6078\n",
            "Epoch 45/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7657 - acc: 0.6840 - val_loss: 1.1017 - val_acc: 0.6536\n",
            "Epoch 46/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7599 - acc: 0.6905 - val_loss: 0.7967 - val_acc: 0.6013\n",
            "Epoch 47/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7612 - acc: 0.6840 - val_loss: 0.9185 - val_acc: 0.6013\n",
            "Epoch 48/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8054 - acc: 0.7100 - val_loss: 1.1173 - val_acc: 0.6405\n",
            "Epoch 49/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7632 - acc: 0.6840 - val_loss: 1.3820 - val_acc: 0.6275\n",
            "Epoch 50/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7919 - acc: 0.6991 - val_loss: 0.8382 - val_acc: 0.5490\n",
            "Epoch 51/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8365 - acc: 0.6775 - val_loss: 0.7095 - val_acc: 0.6536\n",
            "Epoch 52/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8053 - acc: 0.7035 - val_loss: 0.8043 - val_acc: 0.6275\n",
            "Epoch 53/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7697 - acc: 0.6861 - val_loss: 1.0955 - val_acc: 0.6797\n",
            "Epoch 54/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7328 - acc: 0.6948 - val_loss: 0.9276 - val_acc: 0.6405\n",
            "Epoch 55/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7363 - acc: 0.7035 - val_loss: 1.1745 - val_acc: 0.6993\n",
            "Epoch 56/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7740 - acc: 0.7165 - val_loss: 1.0508 - val_acc: 0.6536\n",
            "Epoch 57/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7269 - acc: 0.6818 - val_loss: 0.7929 - val_acc: 0.6601\n",
            "Epoch 58/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7959 - acc: 0.6818 - val_loss: 0.7540 - val_acc: 0.6732\n",
            "Epoch 59/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7705 - acc: 0.7078 - val_loss: 0.7469 - val_acc: 0.5882\n",
            "Epoch 60/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7154 - acc: 0.6970 - val_loss: 0.7640 - val_acc: 0.6405\n",
            "Epoch 61/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7777 - acc: 0.6797 - val_loss: 0.8301 - val_acc: 0.6928\n",
            "Epoch 62/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7053 - acc: 0.6775 - val_loss: 0.9253 - val_acc: 0.6275\n",
            "Epoch 63/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7760 - acc: 0.7121 - val_loss: 0.8384 - val_acc: 0.6471\n",
            "Epoch 64/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7603 - acc: 0.7035 - val_loss: 0.8020 - val_acc: 0.6275\n",
            "Epoch 65/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7888 - acc: 0.6667 - val_loss: 1.0017 - val_acc: 0.6732\n",
            "Epoch 66/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8058 - acc: 0.6948 - val_loss: 0.7243 - val_acc: 0.6209\n",
            "Epoch 67/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8585 - acc: 0.7229 - val_loss: 0.7953 - val_acc: 0.6863\n",
            "Epoch 68/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8063 - acc: 0.6883 - val_loss: 0.9606 - val_acc: 0.6405\n",
            "Epoch 69/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8205 - acc: 0.6970 - val_loss: 0.9363 - val_acc: 0.6275\n",
            "Epoch 70/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8305 - acc: 0.6970 - val_loss: 0.7354 - val_acc: 0.6536\n",
            "Epoch 71/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7424 - acc: 0.6775 - val_loss: 0.7585 - val_acc: 0.6275\n",
            "Epoch 72/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7871 - acc: 0.6883 - val_loss: 0.7471 - val_acc: 0.6863\n",
            "Epoch 73/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7898 - acc: 0.6645 - val_loss: 1.1110 - val_acc: 0.6078\n",
            "Epoch 74/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7611 - acc: 0.6840 - val_loss: 0.7452 - val_acc: 0.6732\n",
            "Epoch 75/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7330 - acc: 0.6623 - val_loss: 0.9072 - val_acc: 0.6667\n",
            "Epoch 76/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7838 - acc: 0.7035 - val_loss: 1.4937 - val_acc: 0.6471\n",
            "Epoch 77/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7739 - acc: 0.6905 - val_loss: 0.9404 - val_acc: 0.6536\n",
            "Epoch 78/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7615 - acc: 0.7143 - val_loss: 0.7607 - val_acc: 0.6863\n",
            "Epoch 79/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8516 - acc: 0.6732 - val_loss: 0.9466 - val_acc: 0.6601\n",
            "Epoch 80/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7541 - acc: 0.6948 - val_loss: 0.9116 - val_acc: 0.6536\n",
            "Epoch 81/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7436 - acc: 0.6667 - val_loss: 0.8020 - val_acc: 0.6144\n",
            "Epoch 82/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7388 - acc: 0.6840 - val_loss: 0.8363 - val_acc: 0.6601\n",
            "Epoch 83/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8409 - acc: 0.6537 - val_loss: 0.9678 - val_acc: 0.6797\n",
            "Epoch 84/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7327 - acc: 0.6797 - val_loss: 0.9349 - val_acc: 0.6732\n",
            "Epoch 85/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7547 - acc: 0.6883 - val_loss: 1.0539 - val_acc: 0.5425\n",
            "Epoch 86/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8215 - acc: 0.6645 - val_loss: 1.0923 - val_acc: 0.6601\n",
            "Epoch 87/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7279 - acc: 0.7078 - val_loss: 1.4193 - val_acc: 0.5948\n",
            "Epoch 88/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7395 - acc: 0.6818 - val_loss: 1.1459 - val_acc: 0.6732\n",
            "Epoch 89/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.6845 - acc: 0.6948 - val_loss: 1.2611 - val_acc: 0.5882\n",
            "Epoch 90/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7166 - acc: 0.7013 - val_loss: 1.1164 - val_acc: 0.6667\n",
            "Epoch 91/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7596 - acc: 0.6623 - val_loss: 1.0383 - val_acc: 0.6732\n",
            "Epoch 92/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8280 - acc: 0.6797 - val_loss: 1.0297 - val_acc: 0.6536\n",
            "Epoch 93/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8058 - acc: 0.7078 - val_loss: 0.8569 - val_acc: 0.6667\n",
            "Epoch 94/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7156 - acc: 0.6818 - val_loss: 1.1975 - val_acc: 0.6078\n",
            "Epoch 95/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7764 - acc: 0.6753 - val_loss: 0.8876 - val_acc: 0.6536\n",
            "Epoch 96/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7744 - acc: 0.7035 - val_loss: 0.9563 - val_acc: 0.6340\n",
            "Epoch 97/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8880 - acc: 0.7165 - val_loss: 0.9416 - val_acc: 0.6209\n",
            "Epoch 98/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7645 - acc: 0.7229 - val_loss: 1.0084 - val_acc: 0.5294\n",
            "Epoch 99/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7875 - acc: 0.7013 - val_loss: 0.8308 - val_acc: 0.6209\n",
            "Epoch 100/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8664 - acc: 0.6883 - val_loss: 0.9125 - val_acc: 0.6144\n",
            "running process 3/4 :\n",
            "Epoch 1/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 5.4424 - acc: 0.5476 - val_loss: 2.5773 - val_acc: 0.4575\n",
            "Epoch 2/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 2.0853 - acc: 0.5974 - val_loss: 0.8078 - val_acc: 0.6340\n",
            "Epoch 3/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 1.3728 - acc: 0.5931 - val_loss: 0.8935 - val_acc: 0.5686\n",
            "Epoch 4/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 1.0682 - acc: 0.6126 - val_loss: 1.1054 - val_acc: 0.6209\n",
            "Epoch 5/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.9617 - acc: 0.6342 - val_loss: 0.7770 - val_acc: 0.7059\n",
            "Epoch 6/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.9523 - acc: 0.6190 - val_loss: 0.7175 - val_acc: 0.7059\n",
            "Epoch 7/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8336 - acc: 0.6299 - val_loss: 0.8390 - val_acc: 0.6144\n",
            "Epoch 8/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.8670 - acc: 0.6667 - val_loss: 0.8029 - val_acc: 0.6536\n",
            "Epoch 9/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.8662 - acc: 0.6277 - val_loss: 0.7152 - val_acc: 0.6732\n",
            "Epoch 10/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8022 - acc: 0.6602 - val_loss: 1.1707 - val_acc: 0.6405\n",
            "Epoch 11/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8738 - acc: 0.6645 - val_loss: 0.7866 - val_acc: 0.6928\n",
            "Epoch 12/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8653 - acc: 0.6645 - val_loss: 0.8109 - val_acc: 0.6797\n",
            "Epoch 13/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8540 - acc: 0.6515 - val_loss: 0.6861 - val_acc: 0.6667\n",
            "Epoch 14/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.8452 - acc: 0.6234 - val_loss: 0.6758 - val_acc: 0.6209\n",
            "Epoch 15/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7847 - acc: 0.6732 - val_loss: 0.7426 - val_acc: 0.6667\n",
            "Epoch 16/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7626 - acc: 0.6407 - val_loss: 0.7039 - val_acc: 0.6405\n",
            "Epoch 17/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8042 - acc: 0.6580 - val_loss: 0.6723 - val_acc: 0.6863\n",
            "Epoch 18/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7869 - acc: 0.6472 - val_loss: 0.7492 - val_acc: 0.5425\n",
            "Epoch 19/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8106 - acc: 0.6667 - val_loss: 0.7202 - val_acc: 0.6275\n",
            "Epoch 20/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7481 - acc: 0.6602 - val_loss: 0.7107 - val_acc: 0.6732\n",
            "Epoch 21/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8112 - acc: 0.6299 - val_loss: 0.7536 - val_acc: 0.5163\n",
            "Epoch 22/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8540 - acc: 0.6537 - val_loss: 0.7828 - val_acc: 0.6405\n",
            "Epoch 23/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8333 - acc: 0.6494 - val_loss: 0.7311 - val_acc: 0.6601\n",
            "Epoch 24/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8057 - acc: 0.6602 - val_loss: 0.7199 - val_acc: 0.6667\n",
            "Epoch 25/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8231 - acc: 0.6797 - val_loss: 0.8089 - val_acc: 0.4706\n",
            "Epoch 26/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8340 - acc: 0.6623 - val_loss: 0.6701 - val_acc: 0.6144\n",
            "Epoch 27/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7916 - acc: 0.6775 - val_loss: 0.7277 - val_acc: 0.6536\n",
            "Epoch 28/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7698 - acc: 0.6753 - val_loss: 0.6508 - val_acc: 0.6993\n",
            "Epoch 29/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7984 - acc: 0.6710 - val_loss: 0.7080 - val_acc: 0.5882\n",
            "Epoch 30/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7888 - acc: 0.6537 - val_loss: 0.7535 - val_acc: 0.6471\n",
            "Epoch 31/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8312 - acc: 0.6797 - val_loss: 0.7163 - val_acc: 0.6601\n",
            "Epoch 32/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7800 - acc: 0.6797 - val_loss: 0.6921 - val_acc: 0.7059\n",
            "Epoch 33/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7460 - acc: 0.6840 - val_loss: 1.1416 - val_acc: 0.6405\n",
            "Epoch 34/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7878 - acc: 0.6667 - val_loss: 0.6918 - val_acc: 0.7124\n",
            "Epoch 35/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7257 - acc: 0.6645 - val_loss: 0.7377 - val_acc: 0.6797\n",
            "Epoch 36/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8000 - acc: 0.6753 - val_loss: 0.7056 - val_acc: 0.6601\n",
            "Epoch 37/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7702 - acc: 0.6775 - val_loss: 0.7408 - val_acc: 0.6928\n",
            "Epoch 38/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.8124 - acc: 0.6623 - val_loss: 0.8013 - val_acc: 0.6732\n",
            "Epoch 39/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7676 - acc: 0.6667 - val_loss: 0.7441 - val_acc: 0.6863\n",
            "Epoch 40/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8267 - acc: 0.6883 - val_loss: 0.7776 - val_acc: 0.6078\n",
            "Epoch 41/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7506 - acc: 0.6775 - val_loss: 0.7701 - val_acc: 0.6144\n",
            "Epoch 42/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8185 - acc: 0.6710 - val_loss: 0.6903 - val_acc: 0.6928\n",
            "Epoch 43/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7919 - acc: 0.6818 - val_loss: 0.6880 - val_acc: 0.6732\n",
            "Epoch 44/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7901 - acc: 0.6883 - val_loss: 0.7013 - val_acc: 0.6275\n",
            "Epoch 45/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7852 - acc: 0.6732 - val_loss: 0.7251 - val_acc: 0.6993\n",
            "Epoch 46/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7627 - acc: 0.6580 - val_loss: 0.8098 - val_acc: 0.6797\n",
            "Epoch 47/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7949 - acc: 0.6667 - val_loss: 1.1263 - val_acc: 0.6928\n",
            "Epoch 48/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.8486 - acc: 0.6667 - val_loss: 0.7282 - val_acc: 0.6340\n",
            "Epoch 49/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8080 - acc: 0.6645 - val_loss: 0.8380 - val_acc: 0.6536\n",
            "Epoch 50/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7556 - acc: 0.6558 - val_loss: 0.8306 - val_acc: 0.6732\n",
            "Epoch 51/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8534 - acc: 0.6688 - val_loss: 0.7268 - val_acc: 0.5948\n",
            "Epoch 52/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7569 - acc: 0.6905 - val_loss: 0.7107 - val_acc: 0.6536\n",
            "Epoch 53/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8097 - acc: 0.6732 - val_loss: 1.7994 - val_acc: 0.6209\n",
            "Epoch 54/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8222 - acc: 0.6753 - val_loss: 0.8945 - val_acc: 0.4967\n",
            "Epoch 55/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7893 - acc: 0.6688 - val_loss: 1.2400 - val_acc: 0.6275\n",
            "Epoch 56/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7797 - acc: 0.6818 - val_loss: 0.9252 - val_acc: 0.6732\n",
            "Epoch 57/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8555 - acc: 0.7013 - val_loss: 0.9917 - val_acc: 0.6667\n",
            "Epoch 58/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8263 - acc: 0.6991 - val_loss: 0.8159 - val_acc: 0.6536\n",
            "Epoch 59/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8035 - acc: 0.6840 - val_loss: 0.8059 - val_acc: 0.5294\n",
            "Epoch 60/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7801 - acc: 0.6991 - val_loss: 0.9898 - val_acc: 0.7320\n",
            "Epoch 61/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8714 - acc: 0.6753 - val_loss: 0.7105 - val_acc: 0.6601\n",
            "Epoch 62/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7906 - acc: 0.6645 - val_loss: 0.6419 - val_acc: 0.7386\n",
            "Epoch 63/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7552 - acc: 0.6688 - val_loss: 0.7154 - val_acc: 0.6928\n",
            "Epoch 64/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8300 - acc: 0.6623 - val_loss: 0.6837 - val_acc: 0.6928\n",
            "Epoch 65/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.8382 - acc: 0.6753 - val_loss: 0.7092 - val_acc: 0.6405\n",
            "Epoch 66/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8310 - acc: 0.6861 - val_loss: 0.6903 - val_acc: 0.6667\n",
            "Epoch 67/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7486 - acc: 0.7013 - val_loss: 0.7996 - val_acc: 0.7516\n",
            "Epoch 68/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7937 - acc: 0.6537 - val_loss: 0.7069 - val_acc: 0.6928\n",
            "Epoch 69/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7739 - acc: 0.6775 - val_loss: 0.7072 - val_acc: 0.6732\n",
            "Epoch 70/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8132 - acc: 0.6840 - val_loss: 0.7553 - val_acc: 0.6536\n",
            "Epoch 71/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7638 - acc: 0.6970 - val_loss: 0.7152 - val_acc: 0.6405\n",
            "Epoch 72/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8536 - acc: 0.6840 - val_loss: 0.6866 - val_acc: 0.6471\n",
            "Epoch 73/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7615 - acc: 0.6753 - val_loss: 0.6902 - val_acc: 0.6993\n",
            "Epoch 74/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8411 - acc: 0.6775 - val_loss: 0.7243 - val_acc: 0.6993\n",
            "Epoch 75/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8102 - acc: 0.6623 - val_loss: 0.8098 - val_acc: 0.7059\n",
            "Epoch 76/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7686 - acc: 0.6905 - val_loss: 0.8646 - val_acc: 0.6340\n",
            "Epoch 77/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8043 - acc: 0.6926 - val_loss: 0.7519 - val_acc: 0.6797\n",
            "Epoch 78/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8392 - acc: 0.6861 - val_loss: 0.7605 - val_acc: 0.6275\n",
            "Epoch 79/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8196 - acc: 0.6472 - val_loss: 0.8165 - val_acc: 0.5425\n",
            "Epoch 80/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8076 - acc: 0.6688 - val_loss: 0.7987 - val_acc: 0.6471\n",
            "Epoch 81/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8157 - acc: 0.6494 - val_loss: 0.7353 - val_acc: 0.6732\n",
            "Epoch 82/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7892 - acc: 0.7078 - val_loss: 1.1932 - val_acc: 0.6471\n",
            "Epoch 83/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7877 - acc: 0.6732 - val_loss: 0.7535 - val_acc: 0.6601\n",
            "Epoch 84/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8589 - acc: 0.6732 - val_loss: 0.7287 - val_acc: 0.6601\n",
            "Epoch 85/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8194 - acc: 0.6861 - val_loss: 0.9541 - val_acc: 0.7124\n",
            "Epoch 86/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8260 - acc: 0.6450 - val_loss: 0.8970 - val_acc: 0.6601\n",
            "Epoch 87/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8094 - acc: 0.6970 - val_loss: 0.7304 - val_acc: 0.6863\n",
            "Epoch 88/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8335 - acc: 0.6623 - val_loss: 0.8518 - val_acc: 0.7059\n",
            "Epoch 89/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.9320 - acc: 0.6926 - val_loss: 0.7906 - val_acc: 0.6471\n",
            "Epoch 90/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8342 - acc: 0.7078 - val_loss: 1.0818 - val_acc: 0.6667\n",
            "Epoch 91/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8621 - acc: 0.6710 - val_loss: 0.8479 - val_acc: 0.5359\n",
            "Epoch 92/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7604 - acc: 0.6602 - val_loss: 1.0455 - val_acc: 0.6536\n",
            "Epoch 93/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8554 - acc: 0.6753 - val_loss: 0.7720 - val_acc: 0.5882\n",
            "Epoch 94/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8308 - acc: 0.7013 - val_loss: 0.7545 - val_acc: 0.6732\n",
            "Epoch 95/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7944 - acc: 0.6667 - val_loss: 0.8721 - val_acc: 0.6993\n",
            "Epoch 96/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8168 - acc: 0.6558 - val_loss: 0.8356 - val_acc: 0.6797\n",
            "Epoch 97/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8072 - acc: 0.6645 - val_loss: 0.9889 - val_acc: 0.7059\n",
            "Epoch 98/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8138 - acc: 0.6797 - val_loss: 1.0825 - val_acc: 0.6209\n",
            "Epoch 99/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8177 - acc: 0.6602 - val_loss: 0.8018 - val_acc: 0.5621\n",
            "Epoch 100/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.8548 - acc: 0.6602 - val_loss: 0.8151 - val_acc: 0.5229\n",
            "running process 4/4 :\n",
            "Epoch 1/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 4.5888 - acc: 0.5411 - val_loss: 1.9407 - val_acc: 0.6601\n",
            "Epoch 2/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 1.8255 - acc: 0.5974 - val_loss: 0.6727 - val_acc: 0.7320\n",
            "Epoch 3/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 1.3420 - acc: 0.6061 - val_loss: 0.7749 - val_acc: 0.6013\n",
            "Epoch 4/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 1.0426 - acc: 0.6342 - val_loss: 0.6609 - val_acc: 0.6471\n",
            "Epoch 5/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.9786 - acc: 0.6017 - val_loss: 0.7893 - val_acc: 0.6275\n",
            "Epoch 6/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.9121 - acc: 0.6169 - val_loss: 1.1364 - val_acc: 0.4183\n",
            "Epoch 7/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.9225 - acc: 0.6472 - val_loss: 0.9224 - val_acc: 0.4183\n",
            "Epoch 8/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8817 - acc: 0.6494 - val_loss: 0.6481 - val_acc: 0.7582\n",
            "Epoch 9/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8701 - acc: 0.6688 - val_loss: 0.9012 - val_acc: 0.4575\n",
            "Epoch 10/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8665 - acc: 0.6342 - val_loss: 0.6802 - val_acc: 0.6078\n",
            "Epoch 11/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8321 - acc: 0.6277 - val_loss: 0.6446 - val_acc: 0.6863\n",
            "Epoch 12/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7756 - acc: 0.6429 - val_loss: 0.5996 - val_acc: 0.7778\n",
            "Epoch 13/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8247 - acc: 0.6580 - val_loss: 0.6280 - val_acc: 0.7255\n",
            "Epoch 14/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8182 - acc: 0.6861 - val_loss: 0.6613 - val_acc: 0.6863\n",
            "Epoch 15/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8402 - acc: 0.6515 - val_loss: 0.6829 - val_acc: 0.7582\n",
            "Epoch 16/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8193 - acc: 0.6407 - val_loss: 0.6683 - val_acc: 0.7516\n",
            "Epoch 17/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8076 - acc: 0.6234 - val_loss: 0.7048 - val_acc: 0.6013\n",
            "Epoch 18/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7783 - acc: 0.6537 - val_loss: 0.7877 - val_acc: 0.4118\n",
            "Epoch 19/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7698 - acc: 0.6277 - val_loss: 0.6560 - val_acc: 0.6732\n",
            "Epoch 20/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7535 - acc: 0.6732 - val_loss: 0.6224 - val_acc: 0.7712\n",
            "Epoch 21/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8127 - acc: 0.6299 - val_loss: 0.7626 - val_acc: 0.4902\n",
            "Epoch 22/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8015 - acc: 0.6407 - val_loss: 0.6947 - val_acc: 0.6732\n",
            "Epoch 23/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8172 - acc: 0.6753 - val_loss: 0.8475 - val_acc: 0.6536\n",
            "Epoch 24/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8963 - acc: 0.6299 - val_loss: 0.6705 - val_acc: 0.6928\n",
            "Epoch 25/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8293 - acc: 0.6429 - val_loss: 0.7455 - val_acc: 0.4837\n",
            "Epoch 26/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7771 - acc: 0.6558 - val_loss: 0.6935 - val_acc: 0.5817\n",
            "Epoch 27/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7579 - acc: 0.6234 - val_loss: 0.7008 - val_acc: 0.7712\n",
            "Epoch 28/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8191 - acc: 0.6580 - val_loss: 0.7266 - val_acc: 0.5425\n",
            "Epoch 29/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7763 - acc: 0.6558 - val_loss: 0.8084 - val_acc: 0.4706\n",
            "Epoch 30/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7873 - acc: 0.6797 - val_loss: 0.6585 - val_acc: 0.7386\n",
            "Epoch 31/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7909 - acc: 0.6537 - val_loss: 0.6601 - val_acc: 0.7647\n",
            "Epoch 32/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7790 - acc: 0.6472 - val_loss: 0.7465 - val_acc: 0.5621\n",
            "Epoch 33/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7460 - acc: 0.6861 - val_loss: 0.8006 - val_acc: 0.7124\n",
            "Epoch 34/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.8322 - acc: 0.6429 - val_loss: 0.7347 - val_acc: 0.6732\n",
            "Epoch 35/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.7975 - acc: 0.6732 - val_loss: 0.8104 - val_acc: 0.4706\n",
            "Epoch 36/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.8060 - acc: 0.6450 - val_loss: 0.6542 - val_acc: 0.6732\n",
            "Epoch 37/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.8186 - acc: 0.6385 - val_loss: 0.7072 - val_acc: 0.6340\n",
            "Epoch 38/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.8077 - acc: 0.6645 - val_loss: 0.6419 - val_acc: 0.7582\n",
            "Epoch 39/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.7726 - acc: 0.6775 - val_loss: 0.7258 - val_acc: 0.5752\n",
            "Epoch 40/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.7722 - acc: 0.6645 - val_loss: 0.6436 - val_acc: 0.7255\n",
            "Epoch 41/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.8227 - acc: 0.6645 - val_loss: 0.6502 - val_acc: 0.7386\n",
            "Epoch 42/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.7945 - acc: 0.6494 - val_loss: 0.7928 - val_acc: 0.5817\n",
            "Epoch 43/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.7581 - acc: 0.6905 - val_loss: 0.6309 - val_acc: 0.7582\n",
            "Epoch 44/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7833 - acc: 0.6710 - val_loss: 0.7343 - val_acc: 0.7124\n",
            "Epoch 45/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8809 - acc: 0.6515 - val_loss: 0.6663 - val_acc: 0.5882\n",
            "Epoch 46/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7661 - acc: 0.6515 - val_loss: 0.9876 - val_acc: 0.4248\n",
            "Epoch 47/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7326 - acc: 0.6861 - val_loss: 1.0558 - val_acc: 0.7712\n",
            "Epoch 48/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8432 - acc: 0.6688 - val_loss: 0.6542 - val_acc: 0.7190\n",
            "Epoch 49/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8134 - acc: 0.6667 - val_loss: 0.6551 - val_acc: 0.7059\n",
            "Epoch 50/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8738 - acc: 0.6861 - val_loss: 0.6447 - val_acc: 0.7908\n",
            "Epoch 51/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7982 - acc: 0.6688 - val_loss: 0.6805 - val_acc: 0.7778\n",
            "Epoch 52/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8041 - acc: 0.6818 - val_loss: 0.6259 - val_acc: 0.7712\n",
            "Epoch 53/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7730 - acc: 0.6623 - val_loss: 0.7794 - val_acc: 0.6797\n",
            "Epoch 54/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8052 - acc: 0.6515 - val_loss: 0.6300 - val_acc: 0.7647\n",
            "Epoch 55/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8057 - acc: 0.6429 - val_loss: 0.6564 - val_acc: 0.7451\n",
            "Epoch 56/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7877 - acc: 0.6667 - val_loss: 0.5677 - val_acc: 0.7582\n",
            "Epoch 57/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8183 - acc: 0.6926 - val_loss: 0.6150 - val_acc: 0.7451\n",
            "Epoch 58/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7790 - acc: 0.6580 - val_loss: 0.6596 - val_acc: 0.7320\n",
            "Epoch 59/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7954 - acc: 0.6797 - val_loss: 0.7528 - val_acc: 0.6928\n",
            "Epoch 60/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8614 - acc: 0.6364 - val_loss: 0.6505 - val_acc: 0.7255\n",
            "Epoch 61/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8224 - acc: 0.6710 - val_loss: 0.7172 - val_acc: 0.7778\n",
            "Epoch 62/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8220 - acc: 0.6688 - val_loss: 0.7007 - val_acc: 0.6993\n",
            "Epoch 63/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7715 - acc: 0.6515 - val_loss: 0.9639 - val_acc: 0.4314\n",
            "Epoch 64/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7966 - acc: 0.6840 - val_loss: 0.8831 - val_acc: 0.3464\n",
            "Epoch 65/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8545 - acc: 0.6905 - val_loss: 0.7054 - val_acc: 0.4706\n",
            "Epoch 66/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8040 - acc: 0.6494 - val_loss: 0.6603 - val_acc: 0.7386\n",
            "Epoch 67/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8457 - acc: 0.6732 - val_loss: 0.8674 - val_acc: 0.4967\n",
            "Epoch 68/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8542 - acc: 0.6667 - val_loss: 0.7773 - val_acc: 0.4902\n",
            "Epoch 69/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7683 - acc: 0.6602 - val_loss: 0.7647 - val_acc: 0.3007\n",
            "Epoch 70/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8615 - acc: 0.6883 - val_loss: 0.6979 - val_acc: 0.6536\n",
            "Epoch 71/100\n",
            "462/462 [==============================] - 1s 3ms/step - loss: 0.8652 - acc: 0.6515 - val_loss: 0.7148 - val_acc: 0.6275\n",
            "Epoch 72/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8381 - acc: 0.6883 - val_loss: 0.6799 - val_acc: 0.7190\n",
            "Epoch 73/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.9091 - acc: 0.6623 - val_loss: 0.6995 - val_acc: 0.6797\n",
            "Epoch 74/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8291 - acc: 0.6602 - val_loss: 0.6048 - val_acc: 0.7647\n",
            "Epoch 75/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8387 - acc: 0.6926 - val_loss: 0.9071 - val_acc: 0.5490\n",
            "Epoch 76/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8683 - acc: 0.6797 - val_loss: 0.8188 - val_acc: 0.6078\n",
            "Epoch 77/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8136 - acc: 0.6732 - val_loss: 0.9409 - val_acc: 0.3464\n",
            "Epoch 78/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8419 - acc: 0.6537 - val_loss: 0.7194 - val_acc: 0.7647\n",
            "Epoch 79/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8343 - acc: 0.6688 - val_loss: 0.5868 - val_acc: 0.7582\n",
            "Epoch 80/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7655 - acc: 0.6645 - val_loss: 0.7287 - val_acc: 0.7124\n",
            "Epoch 81/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8513 - acc: 0.6840 - val_loss: 0.6164 - val_acc: 0.7320\n",
            "Epoch 82/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7669 - acc: 0.6710 - val_loss: 0.6136 - val_acc: 0.7582\n",
            "Epoch 83/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7797 - acc: 0.6580 - val_loss: 0.6801 - val_acc: 0.6993\n",
            "Epoch 84/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8782 - acc: 0.6732 - val_loss: 0.6378 - val_acc: 0.7647\n",
            "Epoch 85/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7444 - acc: 0.6688 - val_loss: 0.6646 - val_acc: 0.6863\n",
            "Epoch 86/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.9404 - acc: 0.6732 - val_loss: 0.6177 - val_acc: 0.7647\n",
            "Epoch 87/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8318 - acc: 0.6472 - val_loss: 0.7433 - val_acc: 0.6275\n",
            "Epoch 88/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8322 - acc: 0.6840 - val_loss: 0.6122 - val_acc: 0.7974\n",
            "Epoch 89/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7982 - acc: 0.6623 - val_loss: 0.6465 - val_acc: 0.7386\n",
            "Epoch 90/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7778 - acc: 0.6623 - val_loss: 0.6769 - val_acc: 0.6732\n",
            "Epoch 91/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7845 - acc: 0.6710 - val_loss: 0.7885 - val_acc: 0.7124\n",
            "Epoch 92/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7894 - acc: 0.7121 - val_loss: 0.6716 - val_acc: 0.6209\n",
            "Epoch 93/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7722 - acc: 0.6775 - val_loss: 0.7213 - val_acc: 0.6993\n",
            "Epoch 94/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7897 - acc: 0.6818 - val_loss: 0.8082 - val_acc: 0.6601\n",
            "Epoch 95/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8666 - acc: 0.6667 - val_loss: 0.7381 - val_acc: 0.6732\n",
            "Epoch 96/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7468 - acc: 0.6688 - val_loss: 0.7528 - val_acc: 0.7255\n",
            "Epoch 97/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7341 - acc: 0.6753 - val_loss: 0.6702 - val_acc: 0.7647\n",
            "Epoch 98/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.7975 - acc: 0.6558 - val_loss: 0.6120 - val_acc: 0.8039\n",
            "Epoch 99/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.8342 - acc: 0.6732 - val_loss: 0.5899 - val_acc: 0.7516\n",
            "Epoch 100/100\n",
            "462/462 [==============================] - 1s 2ms/step - loss: 0.9025 - acc: 0.6861 - val_loss: 0.6809 - val_acc: 0.7386\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju8S_iW2qi4d"
      },
      "source": [
        "average_val_history = [\n",
        "np.mean([x[i] for x in accuracy_val]) for i in range(num_of_epochs)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARUDDKbnsg4X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "aaaefc0d-26a3-47da-e79b-161499f38a73"
      },
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(1, len(average_val_history) + 1), average_val_history) \n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation accuracy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMMYY45UlCGOMMV79P84rN95vyZVFAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ywi3bAWs-cs"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CaLoRTftQ7G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "outputId": "448a9336-0ab1-4a62-b638-373d0d855ece"
      },
      "source": [
        "model = build_model() \n",
        "model.fit(train_data, train_targets,\n",
        "          epochs=100, batch_size=16, verbose=0)\n",
        "loss, accuracy_test = model.evaluate(test_data, test_targets)\n",
        "print(acccuracy_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_get_default_graph\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'get_default_graph'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-1700918dc431>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m model.fit(train_data, train_targets,\n\u001b[1;32m      3\u001b[0m           epochs=100, batch_size=16, verbose=0)\n\u001b[1;32m      4\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macccuracy_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-6d9fb90c3ad5>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m216\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layers, name)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m# Subclassed network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_subclassed_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_base_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_init_subclassed_network\u001b[0;34m(self, name, **kwargs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_subclassed_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expects_training_arg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhas_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_base_init\u001b[0;34m(self, name, trainable, dtype)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_uid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_uid\u001b[0;34m(prefix)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_get_default_graph\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         raise RuntimeError(\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;34m'It looks like you are trying to use '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;34m'a version of multi-backend Keras that '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;34m'does not support TensorFlow 2.0. We recommend '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: It looks like you are trying to use a version of multi-backend Keras that does not support TensorFlow 2.0. We recommend using `tf.keras`, or alternatively, downgrading to TensorFlow 1.14.",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: Current TensorFlow version is 2.2.0-rc1. To use TF 1.x instead,\nrestart your runtime (Ctrl+M .) and run \"%tensorflow_version 1.x\" before\nyou run \"import tensorflow\".\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}